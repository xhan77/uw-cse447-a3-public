{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 3: Deep Transition Dependency Parser\n",
    "\n",
    "In this problem set, you will implement a deep transition dependency parser.\n",
    "\n",
    "You will:\n",
    "\n",
    "- Implement an arc-standard transition-based dependency parser in PyTorch\n",
    "- Implement neural network components for choosing actions and combining stack elements\n",
    "- Train your network to parse English and Norwegian sentences\n",
    "- Implement techniques to improve your parser\n",
    "\n",
    "**ATTENTION**: You should **start early** and **read Eisenstein Ch. 11**. We will cover some key concepts in the lectures, but there are always some technical details to be explored by yourself in the textbook and codebase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup\n",
    "\n",
    "**You can skip the setup section if you have correctly configured the conda environment for Assignment 1 (`mynlpenv`).**\n",
    "\n",
    "You should read [this document](https://docs.google.com/document/d/1iuG6dNRAuhOU7K2ZeLNzIaV1w2InvMGq6VazBzZKFu4/edit?usp=sharing) about computing resources and environments before you start.\n",
    "\n",
    "In order to develop this assignment, you will need [python 3](https://www.python.org/downloads/) (we use 3.8) and the following libraries. Most if not all of these are part of [conda](https://docs.conda.io/en/latest/miniconda.html), so a good starting point would be to install that.\n",
    "\n",
    "- [jupyter](http://jupyter.readthedocs.org/en/latest/install.html)\n",
    "- [numpy](https://numpy.org/)\n",
    "- [matplotlib](http://matplotlib.org/users/installing.html)\n",
    "- [nosetests](https://nose.readthedocs.org/en/latest/)\n",
    "- [pandas](http://pandas.pydata.org/)\n",
    "- [torch](https://pytorch.org/get-started/locally/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are developing on the computing resources we provided to you, you should be able to install all of them at once using the `environment.yml` file we gave you. Simply by running\n",
    "```sh\n",
    "conda env create -f environment.yml\n",
    "```\n",
    "If you are working on other devices, it may be possible the packages we contained in the yaml file being not compatible. In that case, manually create your environment and install the packages according to the document mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this assignment\n",
    "\n",
    "- This is a Jupyter notebook. You can execute cell blocks by pressing control-enter.\n",
    "- Most of your coding will be in the python source files in the directory ```mynlplib```.\n",
    "- The directory ```tests``` contains unit tests that will be used to grade your assignment, using ```nosetests```. You should run them as you work on the assignment to see that you're on the right track. You are free to look at their source code, if that helps -- though most of the relevant code is also here in this notebook.\n",
    "- **To submit this assignment, first read ```submission_guide.md```. In short, you want to add, commit, push, and tag your work in your CSE GitLab repo. Apart from the files you edited in `mynlplib/` and the main Jupyter notebook, you should also submit all the output prediction files (`*.preds`) and the writeup pdf. If you are unsure whether a file should be included or not, ask on Ed.**\n",
    "- We have 76 regular points and 8 possible bonus points for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as ag\n",
    "\n",
    "import nose\n",
    "import numpy as np\n",
    "\n",
    "from imp import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My library versions\n",
      "numpy: 1.21.2\n",
      "nose: 1.3.7\n",
      "torch: 1.8.2\n"
     ]
    }
   ],
   "source": [
    "print('My library versions')\n",
    "\n",
    "print('numpy: {}'.format(np.__version__))\n",
    "print('nose: {}'.format(nose.__version__))\n",
    "print('torch: {}'.format(torch.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test whether your libraries are the right version, run:\n",
    "\n",
    "`nosetests tests/test_environment.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# use ! to run shell commands in notebook\n",
    "! nosetests tests/test_environment.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mynlplib.parsing as parsing\n",
    "import mynlplib.data_tools as data_tools\n",
    "import mynlplib.constants as consts\n",
    "import mynlplib.evaluation as evaluation\n",
    "import mynlplib.utils as utils\n",
    "import mynlplib.feat_extractors as feat_extractors\n",
    "import mynlplib.neural_net as neural_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Read in the datasets\n",
    "reload(data_tools)\n",
    "en_dataset = data_tools.Dataset(consts.EN_TRAIN_FILE, consts.EN_DEV_FILE, consts.EN_TEST_FILE)\n",
    "nr_dataset = data_tools.Dataset(consts.NR_TRAIN_FILE, consts.NR_DEV_FILE, consts.NR_TEST_FILE)\n",
    "\n",
    "# Assign each word a unique index, including two special tokens needed for parsing logic\n",
    "word_to_ix_en = { word: i for i, word in enumerate(en_dataset.vocab) }\n",
    "word_to_ix_nr = { word: i for i, word in enumerate(nr_dataset.vocab) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some constants to keep around\n",
    "LSTM_NUM_LAYERS = 1\n",
    "TEST_EMBEDDING_DIM = 4\n",
    "WORD_EMBEDDING_DIM = 64\n",
    "STACK_EMBEDDING_DIM = 100\n",
    "NUM_FEATURES = 3\n",
    "\n",
    "# Hyperparameters\n",
    "ETA_0 = 0.01\n",
    "DROPOUT = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-Level Overview of the Parser\n",
    "Be sure that you have reviewed Eisenstein Ch. 11.3 on transition-based dependency parsing, and are familiar with the relevant terminology.\n",
    "Parsing will proceed as follows:\n",
    "* Initialize your parsing stack and input buffer.\n",
    "* At each step, until the parse is done:\n",
    "  * Extract some features.  We will start with simple features, but these can be anything: words in the sentence, the configuration of the stack, the configuration of the input buffer, the previous action, etc.\n",
    "  * Send these features through a feed-forward (FF) network to get a probability distribution over actions (`SHIFT`, `ARC_L`, `ARC_R`).  The next action you choose is the one with the highest probability.\n",
    "  * If the action is an arc- operation, you use a neural network to combine the two items in the operation and get a dense output to place back on the input buffer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key classes you will fill in code for are:\n",
    "* Feature extraction in `feat_extractors.py`\n",
    "* The `ParserState` class, which keeps track of the input buffer and parse stack, and offers a public interface for doing the parsing actions to update the state\n",
    "* The `TransitionParser` class, which is a PyTorch module where the core parsing logic resides, in `parsing.py`.\n",
    "* The neural network components in `neural_net.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network components are compartmentalized as follows:\n",
    "* `TransitionParser`, the base component that contains and coordinates the other substitutable components\n",
    "\n",
    "* Embedding Lookup: You will implement three flavors of embeddings. These embeddings are used to initialize the input buffer, and will be shifted on the stack / serve as inputs to the combiner networks.\n",
    "  - `VanillaWordEmbedding` just gets embeddings from a lookup table.\n",
    "  - `BiLSTMWordEmbedding` will run a sequence model in both directions over the sentence. The hidden state at step t is the embedding for the `t`-th word of the sentence.\n",
    "  - `SuffixAndWordEmbedding` gets embeddings for words as in the vanilla embeddings, and also gets embeddings for word suffixes, and concatenates them together.\n",
    "* Action Choosing: You will implement two action choosing components:\n",
    "  - `FFActionChooser` is a simple feed-forward neural network that outputs log probabilities over the three actions given the extracted features as input.\n",
    "  - `LSTMActionChooser` applies a sequence model that takes the hidden state of the previous action decision as input.\n",
    "\n",
    "* Combiners: You will implement two combiners, which are the network components that take the two embeddings of the items in an arc- operation and creates a single vector.\n",
    "  - `FFCombiner` takes the two input embeddings and gives a dense output.\n",
    "  - `LSTMCombiner` applies a sequence model, where the output embedding is the hidden state of the next timestep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is how the input buffer and stack look at each step of a parse, up to the first arc.  The input sentence is \"the dog ran away\".  Our action chooser network takes the top element of the stack, the top element of the input buffer, plus a one-token \"lookahead\" in the input buffer.  $C(x,y)$ refers to calling our combiner network on arguments $x, y$.  Also let $A$ be the set of actions: $\\{ \\text{SHIFT}, \\text{ARC-L}, \\text{ARC-R} \\}$, and let $q_w$ be the embedding for word $w$.\n",
    "\n",
    "1. \n",
    "  * Input Buffer: $\\left[ q_\\text{the}, q_\\text{dog}, q_\\text{ran}, q_\\text{away}, q_\\text{END-INPUT} \\right]$\n",
    "  * Stack: $\\left[ q_\\text{ROOT} \\right]$\n",
    "  * Action: $ \\text{argmax}_{a \\in A} \\ \\text{ActionChooser}(q_\\text{ROOT}, q_\\text{the}, \\overbrace{q_\\text{dog}}^\\text{lookahead}) \\Rightarrow \\text{SHIFT}$\n",
    "  \n",
    "2.\n",
    "  * Input Buffer: $\\left[ q_\\text{dog}, q_\\text{ran}, q_\\text{away}, q_\\text{END-INPUT} \\right]$\n",
    "  * Stack: $\\left[ q_\\text{ROOT}, q_\\text{the} \\right]$\n",
    "  * Action: $ \\text{argmax}_{a \\in A} \\ \\text{ActionChooser}(q_\\text{the}, q_\\text{dog}, q_\\text{ran}) \\Rightarrow \\text{ARC-L}$\n",
    "  \n",
    "3.\n",
    "  * Input Buffer: $\\left[C(q_\\text{dog}, q_\\text{the}), q_\\text{ran}, q_\\text{away}, q_\\text{END-INPUT} \\right]$\n",
    "  * Stack: $\\left[ q_\\text{ROOT} \\right]$\n",
    "  \n",
    "This is a partial picture of parsing - we keep more than just the embedding on the stack and input buffer.  We also keep the word and its position in the sentence so that when we create an arc, we know what edge was just created.\n",
    "So, for example, the initial input buffer really looks like\n",
    "\n",
    "$$ \\left[ (\\text{the}, 0, q_\\text{the}), (\\text{dog}, 1, q_\\text{dog}), (\\text{ran}, 2, q_\\text{ran}), (\\text{away}, 3, q_\\text{away}), (\\text{END-INPUT}, 4, q_\\text{END-INPUT}) \\right] $$\n",
    "\n",
    "Before beginning, we recommend completing the parse by hand, drawing the input buffer and stack at each step, and explicity listing the arguments to the action chooser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Managing and Updating the Parser State (12 points)\n",
    "\n",
    "In this part of the assignment, you will work with the ParserState class, that keeps track of the parser's input buffer and stack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 1.1: Implementing Arc\n",
    "\n",
    "#### 1.1a: Get arc components (2 points)\n",
    "You will implement the generalized arc- operation of the `ParserState` in `parsing.py`, in the function `_arc`, in two parts.\n",
    "\n",
    "First, fill in `_get_arc_components` in `parsing.py` to select the head and modifier according to the action passed in. This method should also remove the items from the stack and input buffer.\n",
    "Arc actions follow the arc-standard procedure, from Eisenstein Ch. 11.3.1.\n",
    "\n",
    "- **Test**: ` test_parser.py:test_get_arc_components_d1_1a`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(parsing)\n",
    "test_sentence = \"The man ran away\".split()\n",
    "parser_state = parsing.ParserState(test_sentence + [consts.END_OF_INPUT_TOK], \n",
    "                                   [None] * (len(test_sentence)+1),\n",
    "                                   utils.DummyCombiner())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack: ['<ROOT>', 'The', 'man']\n",
      "Input Buffer: ['ran', 'away', '<END-OF-INPUT>']\n",
      "\n",
      "StackEntry(headword='ran', headword_pos=2, embedding=None) StackEntry(headword='man', headword_pos=1, embedding=None)\n",
      "StackEntry(headword='The', headword_pos=0, embedding=None) StackEntry(headword='away', headword_pos=3, embedding=None)\n"
     ]
    }
   ],
   "source": [
    "parser_state.shift()\n",
    "parser_state.shift()\n",
    "print(parser_state)\n",
    "\n",
    "head, modifier = parser_state._get_arc_components(consts.Actions.ARC_L)\n",
    "print(head, modifier)\n",
    "\n",
    "head, modifier = parser_state._get_arc_components(consts.Actions.ARC_R)\n",
    "print(head, modifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1b: Create the arc (2 points)\n",
    "Now, fill in `_create_arc` in `parsing.py` to use the `ParserState`'s `combiner` component to **combine** the passed in head and modifier, put the combination on the input buffer, and create a new dependency graph edge. At this point, we are just using a dummy combiner so we can test the logic.\n",
    "\n",
    "You will want to familiarize yourself with the `StackEntry` and `DepGraphEdge` objects used by the `ParserState` object for this one.\n",
    "\n",
    "- **Test**: ` test_parser.py:test_create_arc_d1_1b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack: ['<ROOT>']\n",
      "Input Buffer: ['The', 'man', 'ran', 'away', '<END-OF-INPUT>']\n",
      "\n",
      "Stack: ['<ROOT>', 'The']\n",
      "Input Buffer: ['man', 'ran', 'away', '<END-OF-INPUT>']\n",
      "\n",
      "First arc: Head: ('man', 1), Modifier: ('The', 0) \n",
      "\n",
      "Stack: ['<ROOT>']\n",
      "Input Buffer: ['man', 'ran', 'away', '<END-OF-INPUT>']\n",
      "\n",
      "Second arc: Head: ('ran', 2), Modifier: ('man', 1) \n",
      "\n",
      "Stack: ['<ROOT>']\n",
      "Input Buffer: ['ran', 'away', '<END-OF-INPUT>']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reload(parsing)\n",
    "parser_state = parsing.ParserState(test_sentence + [consts.END_OF_INPUT_TOK], \n",
    "                                   [None] * (len(test_sentence)+1),\n",
    "                                   utils.DummyCombiner())\n",
    "\n",
    "print(parser_state)\n",
    "\n",
    "parser_state.shift()\n",
    "print(parser_state)\n",
    "\n",
    "arc = parser_state.arc_left()\n",
    "print(\"First arc: Head: {}, Modifier: {}\".format(arc[0], arc[1]), \"\\n\")\n",
    "print(parser_state)\n",
    "\n",
    "parser_state.shift()\n",
    "arc = parser_state.arc_left()\n",
    "print(\"Second arc: Head: {}, Modifier: {}\".format(arc[0], arc[1]), \"\\n\")\n",
    "print(parser_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 1.2: Parser Terminating Condition (4 points)\n",
    "In this short (one line) deliverable, implement `done_parsing()` in `ParserState`.  Think about what the input buffer and stack look like at the end of a parse.\n",
    "\n",
    "- **Test**: `test_parsing.py:test_stack_terminating_cond_d1_2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "reload(parsing)\n",
    "parser_state = parsing.ParserState(test_sentence + [consts.END_OF_INPUT_TOK], \n",
    "                                   [None] * (len(test_sentence)+1),\n",
    "                                   utils.DummyCombiner())\n",
    "\n",
    "parser_state.shift()\n",
    "parser_state.arc_left()\n",
    "parser_state.shift()\n",
    "parser_state.arc_left()\n",
    "\n",
    "print(parser_state.done_parsing())\n",
    "\n",
    "parser_state.shift()\n",
    "parser_state.arc_right()\n",
    "print(parser_state.done_parsing())\n",
    "\n",
    "parser_state.arc_right()\n",
    "print(parser_state.done_parsing())\n",
    "\n",
    "parser_state.shift()\n",
    "print(parser_state.done_parsing())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 1.3: Validating parser actions (4 points)\n",
    "Implement the `_validate_action` method in `parsing.TransitionParser`. This will be used in the prediction setting, when the gold standard is not available. We need to ensure that any action we take is legal. Here are the rules:\n",
    "\n",
    "- You cannot shift when the input buffer has <= 2 items on it (including the end of input token), UNLESS the stack is empty.\n",
    "  - **In this case, do `ARC_R` by default.**\n",
    "- You cannot do an arc- operation when the stack is empty (this will happen after creating an arc with ROOT).\n",
    "  - **In this case, do `SHIFT` by default.**\n",
    "- You cannot do an arc-left operation when the root token is on top of the stack.\n",
    "  - **In this case, do `SHIFT` or `ARC-R` depending on the state of the input buffer.**\n",
    "  \n",
    "**Test:**\n",
    "- `test_parser.py:test_validate_action_d1_3`\n",
    "\n",
    "**Make sure you pass the test before you move on. The code blocks below are not meant to be comprehensive.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(parsing)\n",
    "parser_state = parsing.ParserState(test_sentence + [consts.END_OF_INPUT_TOK], \n",
    "                                   [None] * (len(test_sentence)+1),\n",
    "                                   utils.DummyCombiner())\n",
    "ix_to_action = consts.Actions.ix_to_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack: ['<ROOT>']\n",
      "Input Buffer: ['The', 'man', 'ran', 'away', '<END-OF-INPUT>']\n",
      "\n",
      "Chosen action: ARC_L, Valid action: SHIFT\n",
      "\n",
      "Stack: ['<ROOT>', 'The']\n",
      "Input Buffer: ['man', 'ran', 'away', '<END-OF-INPUT>']\n",
      "\n",
      "Chosen action: ARC_L, Valid action: ARC_L\n",
      "\n",
      "Stack: ['<ROOT>', 'The', 'man', 'ran']\n",
      "Input Buffer: ['away', '<END-OF-INPUT>']\n",
      "\n",
      "Chosen action: SHIFT, Valid action: ARC_R\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(parser_state)\n",
    "act_to_do = consts.Actions.ARC_L\n",
    "valid_action = parser_state._validate_action(act_to_do)\n",
    "print(\"Chosen action: %s, Valid action: %s\\n\" % (ix_to_action[act_to_do], ix_to_action[valid_action]))\n",
    "\n",
    "parser_state.shift()\n",
    "\n",
    "print(parser_state)\n",
    "act_to_do = consts.Actions.ARC_L\n",
    "valid_action = parser_state._validate_action(act_to_do)\n",
    "print(\"Chosen action: %s, Valid action: %s\\n\" % (ix_to_action[act_to_do], ix_to_action[valid_action]))\n",
    "\n",
    "parser_state.shift()\n",
    "parser_state.shift()\n",
    "\n",
    "print(parser_state)\n",
    "act_to_do = consts.Actions.SHIFT\n",
    "valid_action = parser_state._validate_action(act_to_do)\n",
    "print(\"Chosen action: %s, Valid action: %s\\n\" % (ix_to_action[act_to_do], ix_to_action[valid_action]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Neural Network for Action Decisions (12 points)\n",
    "In this part of the assignment, you will use PyTorch to create a neural network which examines the current state of the parse and makes the decision to either shift, arc left, or arc right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 2.1: Word Embedding Lookup (3 points)\n",
    "Implement the class `VanillaWordEmbedding` in `neural_net.py`\n",
    "([Docs for Pytorch embeddings](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html))\n",
    "\n",
    "Hint: You will have to turn the input, which is a list of strings (the words in the sentence), into a format that your embedding lookup table can take. \n",
    "\n",
    "**Test:** `test_parser.py:test_word_embed_lookup_d2_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "3 \n",
      "\n",
      "Embedding for 'natural':\n",
      " tensor([[0.6614, 0.2669, 0.0617, 0.6213]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "reload(neural_net)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "test_sentence = \"natural language processing\".split()\n",
    "test_word_to_ix = { \"natural\": 0, \"language\": 1, \"processing\": 2 }\n",
    "\n",
    "word_embedder = neural_net.VanillaWordEmbedding(test_word_to_ix, TEST_EMBEDDING_DIM)\n",
    "embeds = word_embedder(test_sentence)\n",
    "print(type(embeds))\n",
    "print(len(embeds), \"\\n\")\n",
    "print(\"Embedding for 'natural':\\n {}\".format(embeds[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 2.2: Feature Extraction (3 points)\n",
    "Fill in the `SimpleFeatureExtractor` class in `feat_extractors.py` to give the following 3 features as a list **in this order**:\n",
    "* The embedding of the top of the stack\n",
    "* The embedding of the first token in the input buffer\n",
    "* The embedding of the next token in the input buffer (one-token lookahead)\n",
    "\n",
    "If at this point you have not poked around `ParserState` to see how it stores the state, now would be a good time.\n",
    "\n",
    "**Test:** `test_parser.py:test_feature_extraction_d2_2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'The':\n",
      " tensor([[ 0.4391,  1.1712,  1.7674, -0.0954]], grad_fn=<ViewBackward>)\n",
      "Embedding for 'Sound':\n",
      " tensor([[ 0.8657,  0.2444, -0.6629,  0.8073]], grad_fn=<ViewBackward>)\n",
      "Embedding for 'and' (from buffer lookahead):\n",
      " tensor([[ 0.0612, -0.6177, -0.7981, -0.1316]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "reload(feat_extractors)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "test_sentence = \"The Sound and the Fury\".split()\n",
    "test_word_to_ix = { word: i for i, word in enumerate(sorted(set(test_sentence))) }\n",
    "\n",
    "embedder = neural_net.VanillaWordEmbedding(test_word_to_ix, TEST_EMBEDDING_DIM)\n",
    "embeds = embedder(test_sentence)\n",
    "\n",
    "state = parsing.ParserState(test_sentence, embeds, utils.DummyCombiner())\n",
    "\n",
    "state.shift()\n",
    "feat_extractor = feat_extractors.SimpleFeatureExtractor()\n",
    "feats = feat_extractor.get_features(state)\n",
    "\n",
    "print(\"Embedding for 'The':\\n {}\".format(feats[0]))\n",
    "print(\"Embedding for 'Sound':\\n {}\".format(feats[1]))\n",
    "print(\"Embedding for 'and' (from buffer lookahead):\\n {}\".format(feats[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 2.3: Feedforward Network for Choosing Actions (3 points)\n",
    "Implement the class `neural_net.FFActionChooser` according to the specification.\n",
    "You will need to take the list of embeddings passed in (that come from your feature extractor) and concatenate them to one long row vector (size [1 x num actions])\n",
    "\n",
    "This network takes as input the features from your feature extractor, concatenates them, runs them through a feedforward network, and outputs log probabilities over actions.\n",
    "\n",
    "**Test:** `test_parser.py:test_action_chooser_d2_3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2443, -0.8323, -1.2844]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "reload(neural_net)\n",
    "torch.manual_seed(1)\n",
    "act_chooser = neural_net.FFActionChooser(TEST_EMBEDDING_DIM * NUM_FEATURES)\n",
    "feats = [ ag.Variable(torch.randn(1, TEST_EMBEDDING_DIM)) for _ in range(NUM_FEATURES) ] # make some dummy feature embeddings\n",
    "log_probs = act_chooser(feats)\n",
    "print(log_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 2.4: Network for Combining Stack Items (3 points)\n",
    "Implement the class `neural_net.FFCombiner` according to the specification. \n",
    "Recall that what this component does is take two embeddings, the head and modifier, during an arc- operation and output a combined embedding (of size [1 x embedding_dim]), which is then pushed back onto the input buffer during parsing.\n",
    "\n",
    "**Test:** `test_parser.py:test_combiner_d2_4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4285, -0.1363,  0.4046,  0.6006]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "reload(neural_net)\n",
    "torch.manual_seed(1)\n",
    "combiner = neural_net.FFCombiner(TEST_EMBEDDING_DIM)\n",
    "\n",
    "# Again, make dummy inputs\n",
    "head_feat = ag.Variable(torch.randn(1, TEST_EMBEDDING_DIM))\n",
    "modifier_feat = ag.Variable(torch.randn(1, TEST_EMBEDDING_DIM))\n",
    "combined = combiner(head_feat, modifier_feat)\n",
    "print(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Return of the Parser (12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 3.1: Parser Training Code (8 points)\n",
    "We will now complete the parser and train it on our data. It is important to understand the difference between the following tasks:\n",
    "\n",
    "* Training: Training the model involves passing it sentences along with the correct sequence of actions, and updating weights.\n",
    "* Evaluation: We can evaluate the parser by passing it sentences along with the correct sequence of actions, and see how many actions it predicts correctly.  This is identical to training, except the weights are not updated after making a prediction.\n",
    "* Prediction: After setting the weights, we give it a raw sentence (no gold-standard actions), and let it follow its own predicted actions to create a dependency graph, which we can compare to the ground truth.\n",
    "\n",
    "You will implement the `forward()` function in `mynlplib.parsing.TransitionParser`.\n",
    "\n",
    "At this point, it is necessary to have all of the components from part 2 in place for constructing the parser.\n",
    "\n",
    "The parsing logic is roughly as follows:\n",
    "* Loop until parsing state is in its terminating state (deliverable 1.2)\n",
    "* Get the features from the parsing state (deliverable 2.2)\n",
    "* Send them through your action chooser network to get log probabilities over actions (deliverable 2.3)\n",
    "* If you have `gold_actions`, do them. Otherwise (when predicting), take the argmax of your log probabilities, validate the action (deliverable 1.3), and do that. An argmax function is provided for you in `utils.argmax`.\n",
    "\n",
    "Make sure to keep track of the things that the function wants to keep track of\n",
    "* Do all of your actions by calling the appropriate function on your `parser_state`\n",
    "* Append each output autograd.Variable from your action_chooser to the outputs list\n",
    "* Append each action you do to `actions_done`\n",
    "* Build the set of dependency edges as you go\n",
    "\n",
    "**Tests:**\n",
    "- `test_parser.py:test_parse_logic_d3_1`\n",
    "- `test_parser.py:test_predict_after_train_d3_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"The man ran away\".split()\n",
    "test_word_to_ix = { word: i for i, word in enumerate(sorted(set(test_sentence))) }\n",
    "test_word_to_ix[consts.END_OF_INPUT_TOK] = len(test_word_to_ix)\n",
    "test_sentence_vocab = set(test_sentence)\n",
    "gold_actions = [\"SHIFT\", \"ARC_L\", \"SHIFT\", \"ARC_L\", \"SHIFT\", \"ARC_R\", \"ARC_R\", \"SHIFT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{DepGraphEdge(head=('ran', 2), modifier=('away', 3)), DepGraphEdge(head=('ran', 2), modifier=('man', 1)), DepGraphEdge(head=('man', 1), modifier=('The', 0)), DepGraphEdge(head=('<ROOT>', -1), modifier=('ran', 2))}\n",
      "[0, 1, 0, 1, 0, 2, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "reload(parsing)\n",
    "torch.manual_seed(1)\n",
    "feat_extractor = feat_extractors.SimpleFeatureExtractor()\n",
    "word_embedding_lookup = neural_net.VanillaWordEmbedding(test_word_to_ix, STACK_EMBEDDING_DIM)\n",
    "action_chooser = neural_net.FFActionChooser(STACK_EMBEDDING_DIM * NUM_FEATURES)\n",
    "combiner_network = neural_net.FFCombiner(STACK_EMBEDDING_DIM)\n",
    "parser = parsing.TransitionParser(feat_extractor, word_embedding_lookup,\n",
    "                                     action_chooser, combiner_network)\n",
    "output, depgraph, actions_done = parser(test_sentence, gold_actions)\n",
    "print(depgraph)\n",
    "print(actions_done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Train the Parser!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training your parser may take some time. There are 10,000 training sentences. We take a subset of them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_parser(parser, optimizer, dataset, n_epochs=1, n_train_insts=1000):\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"Epoch {}\".format(epoch+1))\n",
    "\n",
    "        parser.train() # turn on dropout layers if they are there\n",
    "        parsing.train(dataset.training_data[:n_train_insts], parser, optimizer, verbose=True)\n",
    "\n",
    "        print(\"Dev Evaluation\")\n",
    "        parser.eval() # turn them off for evaluation\n",
    "        parsing.evaluate(dataset.dev_data, parser, verbose=True)\n",
    "        print(\"F-Score: {}\".format(evaluation.compute_metric(parser, dataset.dev_data, evaluation.fscore)))\n",
    "        print(\"Attachment Score: {}\".format(evaluation.compute_attachment(parser, dataset.dev_data)))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(parsing)\n",
    "torch.manual_seed(1)\n",
    "feat_extractor = feat_extractors.SimpleFeatureExtractor()\n",
    "word_embedding_lookup = neural_net.VanillaWordEmbedding(word_to_ix_en, STACK_EMBEDDING_DIM)\n",
    "action_chooser = neural_net.FFActionChooser(STACK_EMBEDDING_DIM * NUM_FEATURES)\n",
    "combiner_network = neural_net.FFCombiner(STACK_EMBEDDING_DIM)\n",
    "parser = parsing.TransitionParser(feat_extractor, word_embedding_lookup,\n",
    "                                     action_chooser, combiner_network)\n",
    "optimizer = optim.SGD(parser.parameters(), lr=ETA_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances: 100    Number of network actions: 4836\n",
      "Acc: 0.7084367245657568  Loss: 32.68232423067093\n",
      "Number of instances: 100    Number of network actions: 4836\n",
      "Acc: 0.8442928039702233  Loss: 18.603589257597925\n",
      "Number of instances: 100    Number of network actions: 4836\n",
      "Acc: 0.9069478908188585  Loss: 11.905540626049042\n",
      "Number of instances: 100    Number of network actions: 4836\n",
      "Acc: 0.9410669975186104  Loss: 7.805608856528997\n",
      "Number of instances: 100    Number of network actions: 4836\n",
      "Acc: 0.9526468155500414  Loss: 6.317962162379408\n",
      "Number of instances: 100    Number of network actions: 4836\n",
      "Acc: 0.9627791563275434  Loss: 4.578675594367087\n",
      "Number of instances: 100    Number of network actions: 4836\n",
      "Acc: 0.9820099255583127  Loss: 2.974977000351646\n",
      "Number of instances: 100    Number of network actions: 4836\n",
      "Acc: 0.9828370554177006  Loss: 2.4389425828808453\n",
      "2.61 s ± 21.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "torch.manual_seed(1)\n",
    "parsing.train(en_dataset.training_data[:100], parser, optimizer, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Number of instances: 1000    Number of network actions: 44560\n",
      "Acc: 0.8217684021543986  Loss: 21.130796878618188\n",
      "Dev Evaluation\n",
      "Number of instances: 501    Number of network actions: 15846\n",
      "Acc: 0.8315032184778492  Loss: 14.812238732707907\n",
      "F-Score: 0.48959454240639877\n",
      "Attachment Score: 0.47444149943203334\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train the parser for a while here.\n",
    "# Shouldn't take *too* long, even on a laptop\n",
    "torch.manual_seed(1)\n",
    "train_parser(parser, optimizer, en_dataset, n_train_insts=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 3.2: Dev Data Predictions (2 points)\n",
    "Run the code below to output your predictions on the dev data and test data.  You can run the dev test to verify you are correct up to this point.  The test data evaluation is for us, but we won't grade it here this time.\n",
    "\n",
    "**Test**: `test_parser.py:test_dev_d3_2_english`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_sentences = [ sentence for sentence, _ in en_dataset.dev_data ]\n",
    "evaluation.output_preds(consts.EN_D3_2_DEV_FILENAME, parser, dev_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.output_preds(consts.EN_D3_2_TEST_FILENAME, parser, en_dataset.test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 3.3: Dependency parsing in Norwegian (2 points)\n",
    "Run the code below to output your predictions on the **norwegian** dev data and test data.  You can run the dev test to verify you are correct up to this point.  The test data evaluation is for us, but we won't grade it here this time.\n",
    "\n",
    "**Test**: `test_parser.py:test_dev_d3_3_norwegian`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(parsing)\n",
    "torch.manual_seed(1)\n",
    "feat_extractor_nr = feat_extractors.SimpleFeatureExtractor()\n",
    "word_embedding_lookup_nr = neural_net.VanillaWordEmbedding(word_to_ix_nr, STACK_EMBEDDING_DIM)\n",
    "action_chooser_nr = neural_net.FFActionChooser(STACK_EMBEDDING_DIM * NUM_FEATURES)\n",
    "combiner_network_nr = neural_net.FFCombiner(STACK_EMBEDDING_DIM)\n",
    "parser_nr = parsing.TransitionParser(feat_extractor_nr, word_embedding_lookup_nr,\n",
    "                                     action_chooser_nr, combiner_network_nr)\n",
    "optimizer_nr = optim.SGD(parser_nr.parameters(), lr=ETA_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Number of instances: 1000    Number of network actions: 30942\n",
      "Acc: 0.8176588455820567  Loss: 13.477265886433422\n",
      "Dev Evaluation\n",
      "Number of instances: 501    Number of network actions: 16028\n",
      "Acc: 0.8296730721237834  Loss: 13.73407334463853\n",
      "F-Score: 0.46188359209945984\n",
      "Attachment Score: 0.44971300224606936\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "Number of instances: 1000    Number of network actions: 30942\n",
      "Acc: 0.8798073815525822  Loss: 9.107207803953555\n",
      "Dev Evaluation\n",
      "Number of instances: 501    Number of network actions: 16028\n",
      "Acc: 0.8208135762415772  Loss: 17.335564076309577\n",
      "F-Score: 0.46963866667706256\n",
      "Attachment Score: 0.4495882206139256\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "train_parser(parser_nr, optimizer_nr, nr_dataset, n_epochs=2, n_train_insts=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(evaluation)\n",
    "dev_sentences_nr = [ sentence for sentence, _ in nr_dataset.dev_data ]\n",
    "evaluation.output_preds(consts.NR_D3_3_DEV_FILENAME, parser_nr, dev_sentences_nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.output_preds(consts.NR_D3_3_TEST_FILENAME, parser_nr, nr_dataset.test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation and Training Improvements (18 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 4.1: BiLSTM Word Embeddings (3 points)\n",
    "Implement the class `BiLSTMWordEmbedding` in `neural_net.py`.\n",
    "This class can replace your `VanillaWordEmbedding`.\n",
    "This class implements a sequence model over the sentence, where the t'th word's embedding is the hidden state at timestep t.\n",
    "This means that, rather than have our embeddings on the stack only include the semantics of a single word, our embeddings will contain information from all parts of the sentence (the LSTM will, in principle, learn what information is relevant).\n",
    "\n",
    "**Test**: `tests/test_parser.py:test_bilstm_word_embeds_d4_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "2 \n",
      "\n",
      "Embedding for Noam:\n",
      " tensor([[-6.3007e-02,  2.4330e-01, -7.0760e-02, -1.1852e-01,  1.8881e-01,\n",
      "         -1.6543e-01, -2.1600e-02,  1.4040e-02, -6.8058e-02, -1.8666e-01,\n",
      "          1.0207e-01,  2.2894e-02, -5.8540e-02, -6.3337e-02, -2.9607e-01,\n",
      "         -2.0053e-02, -1.8389e-01, -9.1271e-02, -5.1386e-02, -3.4879e-01,\n",
      "         -3.8826e-02,  8.8795e-02, -3.8836e-02,  1.2170e-02,  4.6013e-02,\n",
      "         -1.3923e-01,  1.9091e-02,  7.1751e-02,  9.5653e-02, -3.5629e-01,\n",
      "          1.9788e-01,  2.9786e-02,  6.1633e-02,  4.7286e-02, -2.9223e-01,\n",
      "         -7.4602e-02,  2.4812e-01, -1.3309e-01,  4.2635e-02,  4.2023e-02,\n",
      "          3.1180e-02,  5.5482e-03, -1.1297e-01,  1.4215e-02, -1.0769e-01,\n",
      "         -1.4725e-01, -7.3080e-02,  2.1588e-02,  1.7645e-01,  4.3659e-02,\n",
      "         -2.4070e-04,  1.1204e-02, -2.2866e-01,  1.1086e-01, -3.3928e-02,\n",
      "         -1.3846e-01, -8.5202e-03,  8.6117e-02,  9.5097e-02, -1.2923e-01,\n",
      "         -2.7905e-03, -6.9797e-02,  1.6902e-01, -1.0969e-01, -1.3452e-01,\n",
      "          1.5670e-01,  7.2735e-02, -2.0805e-01, -1.5710e-01, -1.4324e-01,\n",
      "         -7.5958e-02,  1.8515e-01,  4.4450e-02,  7.9908e-02, -1.2914e-01,\n",
      "         -1.5535e-01,  3.2856e-02, -1.4755e-01,  5.6925e-02, -9.1804e-02,\n",
      "         -1.3480e-01, -1.8542e-02,  1.4489e-01,  9.3584e-02,  6.2046e-02,\n",
      "         -9.7762e-02,  8.8054e-02,  6.1904e-02,  1.5637e-02,  5.7491e-02,\n",
      "          3.4077e-02, -1.6989e-01,  3.2679e-03, -1.2580e-01,  7.0331e-02,\n",
      "         -9.3819e-02, -1.1116e-01, -1.8796e-01,  3.5303e-02, -1.0488e-01]],\n",
      "       grad_fn=<UnbindBackward>)\n"
     ]
    }
   ],
   "source": [
    "reload(neural_net)\n",
    "torch.manual_seed(1)\n",
    "test_sentence = \"Noam Chomsky\".split()\n",
    "test_word_to_ix = { \"Noam\": 0, \"Chomsky\": 1 }\n",
    "\n",
    "lstm_word_embedder = neural_net.BiLSTMWordEmbedding(test_word_to_ix,\n",
    "                                                    WORD_EMBEDDING_DIM,\n",
    "                                                    STACK_EMBEDDING_DIM,\n",
    "                                                    num_layers=LSTM_NUM_LAYERS,\n",
    "                                                    dropout=DROPOUT)\n",
    "    \n",
    "lstm_embeds = lstm_word_embedder(test_sentence)\n",
    "print(type(lstm_embeds))\n",
    "print(len(lstm_embeds), \"\\n\")\n",
    "print(\"Embedding for Noam:\\n {}\".format(lstm_embeds[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 4.2: Suffix Embeddings (3 points)\n",
    "We can also try to more explicitly include morphological information by embedding the suffix of a word in addition to the word itself. We approximate the \"suffix\" by just looking at the last two characters of a word.\n",
    "\n",
    "First, implement the function `build_suff_to_ix` in `utils.py`. It should take in a `word_to_ix` lookup and return a `suff_to_ix` lookup.\n",
    "\n",
    "Then, implement the class `SuffixAndWordEmbedding` in `neural_net.py`.\n",
    "This class embeds the words and suffixes in a sentence and then concatenates them to form one embedding. \n",
    "\n",
    "**Test**: `tests/test_parser.py:test_suff_word_embeds_d4_2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "reload(utils)\n",
    "suff_to_ix_en = utils.build_suff_to_ix(word_to_ix_en)\n",
    "suff_to_ix_nr = utils.build_suff_to_ix(word_to_ix_nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1145, 849)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(suff_to_ix_en), len(suff_to_ix_nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(neural_net)\n",
    "torch.manual_seed(1)\n",
    "test_sentence = \"prefix fixsuf fixinfix\".split()\n",
    "test_word_to_ix = { \"prefix\": 0, \"fixsuf\": 1, \"fixinfix\": 2 }\n",
    "test_suff_to_ix = utils.build_suff_to_ix(test_word_to_ix)\n",
    "\n",
    "suff_word_embedder = neural_net.SuffixAndWordEmbedding(test_word_to_ix, test_suff_to_ix, TEST_EMBEDDING_DIM)\n",
    "test_embs = suff_word_embedder(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6614,  0.2669, -1.5228,  0.3817]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_embs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 4.3: Pretrained Embeddings (2 points)\n",
    "\n",
    "Fill in the function `initialize_with_pretrained` in `utils.py`.\n",
    "\n",
    "It will take a word embedding lookup component and initialize its lookup table with pretrained embeddings, which are provided. Note that this is only applicable for the Vanilla, BiLSTM, and SuffixAndWord embedding components.\n",
    "\n",
    "**Test**: `tests/test_parser.py:test_pretrained_embeddings_d4_3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12429751455783844, -0.11472601443529129, -0.5684014558792114, -0.396965891122818, 0.22938089072704315]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "pretrained_embeds = pickle.load(open(consts.PRETRAINED_EMBEDS_FILE, 'rb'))\n",
    "print(pretrained_embeds['four'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "embedder = neural_net.VanillaWordEmbedding(word_to_ix_en,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5021,  1.2408, -0.7810, -1.0325, -0.1347], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder.forward(['four'])[0][0,:5] # For this cell, don't worry about unmatching output. Just make sure to pass the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1243, -0.1147, -0.5684, -0.3970,  0.2294], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "reload(utils);\n",
    "utils.initialize_with_pretrained(pretrained_embeds,embedder)\n",
    "print(embedder.forward(['four'])[0][0,:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 4.4: Better Arc Component Combination (3 points)\n",
    "Before, in order to combine two embeddings during an arc- operation, we just passed them through a feed-forward network and got a dense output.  Now, we will instead use a sequence model of the stack.  The combined embedding from an arc- operation is the next time step of an LSTM.  Implement `neural_net.LSTMCombiner`.\n",
    "\n",
    "**Test**: `tests/test_parser.py:test_lstm_combiner_d4_4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "reload(neural_net)\n",
    "torch.manual_seed(1)\n",
    "combiner = neural_net.LSTMCombiner(TEST_EMBEDDING_DIM,\n",
    "                                          num_layers=LSTM_NUM_LAYERS,\n",
    "                                          dropout=DROPOUT)\n",
    "head_feat = ag.Variable(torch.randn(1,TEST_EMBEDDING_DIM))\n",
    "mod_feat = ag.Variable(torch.randn(1,TEST_EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0532, -0.1534,  0.1484, -0.0595]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined = combiner(head_feat, mod_feat)\n",
    "combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 4.5: Better action choosing (3 points)\n",
    "Instead of choosing the action from the combiner output independently at each time step, let's use an LSTM to predict the action. This way, past actions can influence the current decision directly. \n",
    "\n",
    "Implement `neural_net.LSTMActionChooser`. Use a linear layer to predict the action from the LSTM hidden state.\n",
    "\n",
    "**Test**: `tests/test_parser.py:test_lstm_action_chooser_d4_5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(neural_net)\n",
    "torch.manual_seed(1)\n",
    "action_chooser = neural_net.LSTMActionChooser(TEST_EMBEDDING_DIM * NUM_FEATURES,\n",
    "                                                     LSTM_NUM_LAYERS,\n",
    "                                                     dropout=DROPOUT)\n",
    "feats = [ag.Variable(torch.randn(1, TEST_EMBEDDING_DIM)) for _ in range(NUM_FEATURES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0328, -1.1798, -1.0887]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = action_chooser(feats)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain with the new components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(utils)\n",
    "reload(neural_net)\n",
    "reload(parsing)\n",
    "reload(feat_extractors)\n",
    "torch.manual_seed(1)\n",
    "stack_dim = STACK_EMBEDDING_DIM\n",
    "feat_extractor = feat_extractors.SimpleFeatureExtractor()\n",
    "word_embedding_lookup = neural_net.BiLSTMWordEmbedding(word_to_ix_en,\n",
    "                                                       WORD_EMBEDDING_DIM,\n",
    "                                                       STACK_EMBEDDING_DIM,\n",
    "                                                       num_layers=LSTM_NUM_LAYERS,\n",
    "                                                       dropout=DROPOUT)\n",
    "utils.initialize_with_pretrained(pretrained_embeds, word_embedding_lookup)\n",
    "action_chooser = neural_net.LSTMActionChooser(STACK_EMBEDDING_DIM * NUM_FEATURES,\n",
    "                                              LSTM_NUM_LAYERS,\n",
    "                                              dropout=DROPOUT)\n",
    "combiner = neural_net.LSTMCombiner(STACK_EMBEDDING_DIM,\n",
    "                                   num_layers=LSTM_NUM_LAYERS,\n",
    "                                   dropout=DROPOUT)\n",
    "parser = parsing.TransitionParser(feat_extractor, word_embedding_lookup,\n",
    "                                  action_chooser, combiner)\n",
    "optimizer = optim.SGD(parser.parameters(), lr=ETA_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Number of instances: 1000    Number of network actions: 44560\n",
      "Acc: 0.7991023339317774  Loss: 20.710318389445543\n",
      "Dev Evaluation\n",
      "Number of instances: 501    Number of network actions: 15846\n",
      "Acc: 0.8578821153603433  Loss: 10.88978472700019\n",
      "F-Score: 0.5828719165056494\n",
      "Attachment Score: 0.5544616938028525\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "Number of instances: 1000    Number of network actions: 44560\n",
      "Acc: 0.8856597845601436  Loss: 12.440973245767877\n",
      "Dev Evaluation\n",
      "Number of instances: 501    Number of network actions: 15846\n",
      "Acc: 0.8786444528587656  Loss: 9.406325502964313\n",
      "F-Score: 0.6367727343343508\n",
      "Attachment Score: 0.6081029912911776\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The LSTMs will make this take longer, probably just a few minutes\n",
    "train_parser(parser, optimizer, en_dataset, n_epochs=2, n_train_insts=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 4.6: Dev Predictions: English (2 point)\n",
    "\n",
    "**Test**: `tests/test_parser.py:test_dev_preds_d4_6_english`\n",
    "\n",
    "Run the code below to output your predictions on the dev data and test data.  You can run the dev test to verify you are correct up to this point.  The test data evaluation is for us, but we won't grade it here this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_sentences = [ sentence for sentence, _ in en_dataset.dev_data ]\n",
    "evaluation.output_preds(consts.EN_D4_6_DEV_FILENAME, parser, dev_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.output_preds(consts.EN_D4_6_TEST_FILENAME, parser, en_dataset.test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 4.7: Dev Predictions: Norwegian (2 point)\n",
    "\n",
    "**Test**: `tests/test_parser.py:test_dev_preds_d4_7_norwegian`\n",
    "\n",
    "Run the code below to output your predictions on the dev data and test data.  You can run the dev test to verify you are correct up to this point.  The test data evaluation is for us, but we won't grade it here this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "feat_extractor_nr = feat_extractors.SimpleFeatureExtractor()\n",
    "word_embedding_lookup_nr = neural_net.BiLSTMWordEmbedding(word_to_ix_nr,\n",
    "                                                          WORD_EMBEDDING_DIM,\n",
    "                                                          STACK_EMBEDDING_DIM,\n",
    "                                                          num_layers=LSTM_NUM_LAYERS,\n",
    "                                                          dropout=DROPOUT)\n",
    "action_chooser_nr = neural_net.FFActionChooser(STACK_EMBEDDING_DIM * NUM_FEATURES)\n",
    "combiner_nr = neural_net.LSTMCombiner(STACK_EMBEDDING_DIM,\n",
    "                                          num_layers=LSTM_NUM_LAYERS,\n",
    "                                          dropout=DROPOUT)\n",
    "parser_nr = parsing.TransitionParser(feat_extractor_nr, word_embedding_lookup_nr,\n",
    "                                  action_chooser_nr, combiner_nr)\n",
    "optimizer_nr = optim.SGD(parser_nr.parameters(), lr=ETA_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Number of instances: 1000    Number of network actions: 30942\n",
      "Acc: 0.8109042725098572  Loss: 13.388477278728038\n",
      "Dev Evaluation\n",
      "Number of instances: 501    Number of network actions: 16028\n",
      "Acc: 0.8423384077863738  Loss: 11.7031256609416\n",
      "F-Score: 0.5319294405599143\n",
      "Attachment Score: 0.5017469428500124\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "Number of instances: 1000    Number of network actions: 30942\n",
      "Acc: 0.8821989528795812  Loss: 8.606027949669864\n",
      "Dev Evaluation\n",
      "Number of instances: 501    Number of network actions: 16028\n",
      "Acc: 0.8594334913900674  Loss: 10.45286178637757\n",
      "F-Score: 0.5708863913110733\n",
      "Attachment Score: 0.5450461692038931\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "Number of instances: 1000    Number of network actions: 30942\n",
      "Acc: 0.9164889147437141  Loss: 6.347299872720847\n",
      "Dev Evaluation\n",
      "Number of instances: 501    Number of network actions: 16028\n",
      "Acc: 0.866046917893686  Loss: 10.74305349288735\n",
      "F-Score: 0.5732520484017493\n",
      "Attachment Score: 0.5534065385575243\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_parser(parser_nr, optimizer_nr, nr_dataset, n_epochs=3, n_train_insts=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_sentences_nr = [ sentence for sentence, _ in nr_dataset.dev_data ]\n",
    "evaluation.output_preds(consts.NR_D4_7_DEV_FILENAME, parser_nr, dev_sentences_nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.output_preds(consts.NR_D4_7_TEST_FILENAME, parser_nr, nr_dataset.test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Bakeoff (14 points)\n",
    "\n",
    "We will have another bakeoff for this assignment.\n",
    "\n",
    "Try to implement new features and tune your network's architecture and hyperparameters to get the best network.\n",
    "Section 3 of [this paper](https://arxiv.org/pdf/1206.5533.pdf) may help out with hyperparameter tuning if you are new to neural networks.\n",
    "To get very competitive, it may be necessary to train for a long time (e.g., leaving it running overnight).  Here are some suggestions.\n",
    "* Tune your learning rate and other hyperparameters.\n",
    "* Try different optimizers.  torch.optim has a ton of different training algorithms.  SGD was used in this assignment because it is fast, but it is the most vanilla of them.  Trying new ones, like Adam, will likely boost performance\n",
    "* Try adding regularization to your network if you see evidence that it is overfitting. This can be done with:\n",
    "  * L2 regularization using the weight decay argument\n",
    "  * adding dropout (already an input argument to some of the neural net components)\n",
    "  * implement early stopping (stop training if dev performance on some metric doesn't improve for k epochs)\n",
    "* Try customizing any of the 3 components (word embeddings, action choosing, combining) in clever ways.  You can create new classes that expose the same public interface and use them here (just leave your required ones untouched). Building word embeddings from characters using an RNN or convolutional layer may help.\n",
    "* Try new features.  Write new classes that expose the same public interface as SimpleFeatureExtractor.  Try looking further into stack history, or more input buffer lookahead, or features based on the action sequence. The possibilities are endless.\n",
    "* Within our currect interface, you can use any neural nets and static pretrained word embeddings (no pretrained contextualized embeddings from large data like BERT).\n",
    "\n",
    "**Tests**: \n",
    "- `tests/test_parser.py:test_dev_preds_bakeoff_d5_1_english`\n",
    "- `tests/test_parser.py:test_dev_preds_bakeoff_d5_2_norwegian`\n",
    "- `tests/test_parser.py:test_test_preds_bakeoff_d5_3_english` (hidden)\n",
    "- `tests/test_parser.py:test_test_preds_bakeoff_d5_4_norwegian` (hidden)\n",
    "\n",
    "**Rubric** (attachment score):\n",
    "\n",
    "English dev:\n",
    "- $\\geq$ 0.76: 2 points\n",
    "- $\\geq$ 0.77: 3 points\n",
    "- $\\geq$ 0.78: 4 points\n",
    "\n",
    "Norwegian dev:\n",
    "- $\\geq$ 0.71: 2 points\n",
    "- $\\geq$ 0.72: 3 points\n",
    "- $\\geq$ 0.73: 4 points\n",
    "\n",
    "English test (hidden):\n",
    "- $\\geq$ 0.72: 1 points\n",
    "- $\\geq$ 0.73: 2 points\n",
    "- $\\geq$ 0.74: 3 points\n",
    "\n",
    "Norwegian test (hidden):\n",
    "- $\\geq$ 0.70: 1 points\n",
    "- $\\geq$ 0.71: 2 points\n",
    "- $\\geq$ 0.72: 3 points\n",
    "\n",
    "**Extra credit**:\n",
    "\n",
    "For each language:\n",
    "- Top five test performance in the class (2 points bonus)\n",
    "- We'll also give 2 bonus points to particularly unique / creative / well-motivated solutions (with motivation to be included in the write-up)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Cuda\n",
    "You can use CUDA to train your network, and you should expect decent speedup if you have a GPU and the CUDA toolkit installed.\n",
    "If you want to use CUDA in this assignment, change the HAVE_CUDA variable to True in constants.py, and call `.to_cuda()` on your parser. You may also need to reconfigure your Embedding layers if you didn't consider cuda before. We are not officially supporting CUDA though. If you have problems installing or running CUDA, perhaps just use the CPU. **Reminder: Make sure that you can pass all of the original tests.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your hyperparameters here\n",
    "# e.g learning rate, regularization, lr annealing, dimensionality of embeddings, number of epochs, early stopping etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make your parser here\n",
    "# name your TransitionParser bakeoff_parser to output your predictions below\n",
    "# bakeoff_parser_en = TransitionParser(...)\n",
    "\n",
    "# Also, choose an optimizer.\n",
    "# bakeoff_optimizer_en = optim...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train for bakeoff\n",
    "train_parser(bakeoff_parser_en, bakeoff_optimizer_en, en_dataset, n_epochs=5, n_train_insts=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <<< RUN THIS CELL. IT'S IMPORTANT NOT TO RENAME THE FILE! >>>\n",
    "# write dev output\n",
    "dev_sentences = [ sentence for sentence, _ in en_dataset.dev_data ]\n",
    "evaluation.output_preds(\"bakeoff-dev-en.preds\", bakeoff_parser_en, dev_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev performance\n",
    "evaluation.compute_output_attachment(\"bakeoff-dev-en.preds\", consts.EN_DEV_GOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <<< RUN THIS CELL. IT'S IMPORTANT NOT TO RENAME THE FILE! >>>\n",
    "# write test output\n",
    "evaluation.output_preds(\"bakeoff-test-en.preds\", bakeoff_parser_en, en_dataset.test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now make your norwegian parser if necessary\n",
    "# name your TransitionParser bakeoff_parser to output your predictions below\n",
    "# bakeoff_parser_nr = TransitionParser(...)\n",
    "\n",
    "# Also, choose an optimizer.\n",
    "# bakeoff_optimizer_nr = optim...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train for bakeoff\n",
    "train_parser(bakeoff_parser_nr, bakeoff_optimizer_nr, nr_dataset, n_epochs=5, n_train_insts=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <<< RUN THIS CELL. IT'S IMPORTANT NOT TO RENAME THE FILE! >>>\n",
    "# write dev output\n",
    "dev_sentences_nr = [ sentence for sentence, _ in nr_dataset.dev_data ]\n",
    "evaluation.output_preds(\"bakeoff-dev-nr.preds\", bakeoff_parser_nr, dev_sentences_nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev performance\n",
    "evaluation.compute_output_attachment(\"bakeoff-dev-nr.preds\", consts.NR_DEV_GOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <<< RUN THIS CELL. IT'S IMPORTANT NOT TO RENAME THE FILE! >>>\n",
    "# write test output\n",
    "evaluation.output_preds(\"bakeoff-test-nr.preds\", bakeoff_parser_nr, nr_dataset.test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Writeup (8 points)\n",
    "\n",
    "You can start your write-up in any format you prefer (e.g., LaTeX, Markdown, handwritten if intelligible), but please remember to export to `writeup.pdf` upon submission. You will be encouraged to post or discuss your writeup on Ed *after the due date (plus late days)*.\n",
    "\n",
    "**Deliverable 6.1** (4 points):\n",
    "\n",
    "Briefly describe your bakeoff design.\n",
    "\n",
    "**Deliverable 6.2** (2 points):\n",
    "\n",
    "Consider a sentence of 3 words:\n",
    "- In Assignment 1, we learned to build a text classifier. If the classifier is binary, there are $2$ possible outputs for this sentence.\n",
    "- In Assignment 2, we learned to build a sequence tagger. If each word has two possible labels, there are $2^{3} = 8$ possible outputs for this sentence.\n",
    "- In this project, we learned to build a parser for **projective** dependency trees.\n",
    "- **How many possible projective dependency trees are there for our sentence of 3 words?** (2 points, no need for explanation)\n",
    "- *Bonus*: **How many possible projective dependency trees are there for a sentence of 10 words? Why?** (2 points bonus for a correct number, and 2 points bonus for a proof)\n",
    "\n",
    "**Deliverable 6.3** (2 points):\n",
    "\n",
    "What is your favorite concept learned in this class? If you are to give a quiz question to the next iteration of the class, what would that be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
